

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Hailey">
  <meta name="keywords" content="Notebook">
  
    <meta name="description" content="基于吴恩达机器学习 - Machine Learning Specialization 课程 的笔记第三部分；非监督学习与推荐系统；">
<meta property="og:type" content="article">
<meta property="og:title" content="【课程】吴恩达机器学习课程(三)">
<meta property="og:url" content="http://achlier.github.io/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/index.html">
<meta property="og:site_name" content="Borderland">
<meta property="og:description" content="基于吴恩达机器学习 - Machine Learning Specialization 课程 的笔记第三部分；非监督学习与推荐系统；">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://achlier.github.io/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-1.jpg">
<meta property="og:image" content="http://achlier.github.io/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-2.png">
<meta property="og:image" content="http://achlier.github.io/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-3.jpg">
<meta property="article:published_time" content="2022-08-05T11:47:49.000Z">
<meta property="article:modified_time" content="2022-08-09T20:19:29.360Z">
<meta property="article:author" content="Hailey">
<meta property="article:tag" content="Notebook">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://achlier.github.io/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-1.jpg">
  
  
  
  <title>【课程】吴恩达机器学习课程(三) - Borderland</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"achlier.github.io","root":"/","version":"1.9.0","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":false,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 6.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Achlier&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【课程】吴恩达机器学习课程(三)"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-08-05 19:47" pubdate>
          August 5, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          150 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">【课程】吴恩达机器学习课程(三)</h1>
            
            <div class="markdown-body">
              
              <blockquote>
<p>基于吴恩达机器学习 - Machine Learning Specialization <a target="_blank" rel="noopener" href="https://www.coursera.org/specializations/machine-learning-introduction">课程</a> 的笔记第三部分；非监督学习与推荐系统；</p>
</blockquote>
<span id="more"></span>
<h1 id="Course-3-Unsupervised-Learning-Recommenders-Reinforcement-Learning"><a href="#Course-3-Unsupervised-Learning-Recommenders-Reinforcement-Learning" class="headerlink" title="Course 3 : Unsupervised Learning, Recommenders, Reinforcement Learning"></a>Course 3 : Unsupervised Learning, Recommenders, Reinforcement Learning</h1><p>In the third course of the Machine Learning Specialization, you will:</p>
<ul>
<li>Use unsupervised learning techniques for unsupervised learning: including clustering and anomaly detection</li>
<li>Build recommender systems with a collaborative filtering approach and a content-based deep learning method</li>
<li>Build a deep reinforcement learning model.</li>
</ul>
<h2 id="Week-8-Unsupervised-Learning"><a href="#Week-8-Unsupervised-Learning" class="headerlink" title="Week 8 : Unsupervised Learning"></a>Week 8 : Unsupervised Learning</h2><h3 id="13-Clustering"><a href="#13-Clustering" class="headerlink" title="13. Clustering"></a>13. Clustering</h3><h4 id="13-1-Introduction"><a href="#13-1-Introduction" class="headerlink" title="13.1 Introduction"></a>13.1 Introduction</h4><p>在一个典型的监督学习中，我们有一个带标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的：</p>
<p><img src="/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-1.jpg" srcset="/img/loading.gif" lazyload alt></p>
<p>也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中。我们可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到特定点集的算法，就被称为聚类算法。</p>
<h4 id="13-2-Means-Algorithm"><a href="#13-2-Means-Algorithm" class="headerlink" title="13.2 Means Algorithm"></a>13.2 Means Algorithm</h4><p><strong>K-均值</strong> 是一个迭代算法，假设我们想要将数据聚类成 $K$ 个组，其方法为:</p>
<ol>
<li><p>首先随机选择$K$个随机的点，称为<strong>聚类中心</strong>（<strong>cluster centroids</strong>）</p>
</li>
<li><p>对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类</p>
</li>
<li><p>计算每一个组的平均值</p>
</li>
<li>将该组所关联的中心点移动到平均值的位置</li>
<li>重复步骤2-4直至中心点不再变化。</li>
</ol>
<h5 id="【Python-代码】"><a href="#【Python-代码】" class="headerlink" title="【Python 代码】"></a>【Python 代码】</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_init</span>(<span class="hljs-params">data, k</span>):<br>    <span class="hljs-keyword">return</span> data.sample(k).values<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">combine_data_C</span>(<span class="hljs-params">data, C</span>):<br>    data_with_c = data.copy()<br>    data_with_c[<span class="hljs-string">&#x27;C&#x27;</span>] = C<br>    <span class="hljs-keyword">return</span> data_with_c<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">nearst_cluster</span>(<span class="hljs-params">x, centroids</span>):<br>    distances = np.apply_along_axis(func1d=np.linalg.norm, axis=<span class="hljs-number">1</span>, arr=centroids - x)<br>    <span class="hljs-keyword">return</span> np.argmin(distances)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">assign_cluster</span>(<span class="hljs-params">data, centroids</span>):<br>    <span class="hljs-keyword">return</span> np.apply_along_axis(<span class="hljs-keyword">lambda</span> x: nearst_cluster(x, centroids), axis=<span class="hljs-number">1</span>, arr=data.values)<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">new_centroids</span>(<span class="hljs-params">data, C</span>):<br>    data_with_c = combine_data_C(data, C)<br>    <span class="hljs-keyword">return</span> data_with_c.groupby(<span class="hljs-string">&#x27;C&#x27;</span>, as_index=<span class="hljs-literal">False</span>).mean().sort_values(by=<span class="hljs-string">&#x27;C&#x27;</span>).drop(<span class="hljs-string">&#x27;C&#x27;</span>, axis=<span class="hljs-number">1</span>).values<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cost</span>(<span class="hljs-params">data, centroids, C</span>):<br>    m = data.shape[<span class="hljs-number">0</span>]<br>    data_belong_C = centroids[C]<br>    distances = np.apply_along_axis(func1d=np.linalg.norm, axis=<span class="hljs-number">1</span>, arr=data.values-data_belong_C)<br>    <span class="hljs-keyword">return</span> distances.<span class="hljs-built_in">sum</span>()/m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">k_means_iter</span>(<span class="hljs-params">data, k, epoch=<span class="hljs-number">100</span>, tol=<span class="hljs-number">0.0001</span></span>):<br>    centroids=random_init(data, k)<br>    cost_epoch = []<br>    <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>        C = assign_cluster(data, centroids)<br>        centroids = new_centroids(data, C)<br>        cost_epoch.append(cost(data, centroids, C))<br>        <br>        <span class="hljs-keyword">if</span> i&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> np.<span class="hljs-built_in">abs</span>(cost_epoch[-<span class="hljs-number">1</span>] - cost_epoch[-<span class="hljs-number">2</span>]) / cost_epoch[-<span class="hljs-number">1</span>] &lt; tol:<br>            <span class="hljs-keyword">break</span>;<br>            <br>        <span class="hljs-keyword">return</span> C, centroids, cost_epoch[-<span class="hljs-number">1</span>]<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">k_means</span>(<span class="hljs-params">data, k, epoch=<span class="hljs-number">100</span>, n_init=<span class="hljs-number">10</span></span>):<br>    tries = np.array([k_means_iter(data, k, epoch) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_init)])<br>    least_cost_idx = np.argmin(tries[:, -<span class="hljs-number">1</span>])<br>    <br>    <span class="hljs-keyword">return</span> tries[least_cost_idx]<br></code></pre></td></tr></table></figure>
<h5 id="【Sklearn-Kmeans】"><a href="#【Sklearn-Kmeans】" class="headerlink" title="【Sklearn Kmeans】"></a>【Sklearn Kmeans】</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><br>sk_kmeans = KMeans(n_clusters=<span class="hljs-number">3</span>)<br>sk_kmeans.fit(data2)<br>sk_C = sk_kmeans.predict(data2)<br>data_with_C = combine_data_C(data2, sk_C)<br></code></pre></td></tr></table></figure>
<h4 id="13-3-Optimization-Objective"><a href="#13-3-Optimization-Objective" class="headerlink" title="13.3 Optimization Objective"></a>13.3 Optimization Objective</h4><p>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称<strong>畸变函数</strong> <strong>Distortion function</strong>）为：</p>
<script type="math/tex; mode=display">
J(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\dfrac {1}{m}\sum^{m}_{i=1}\left\| X^{(i)}-\mu_{c^{(i)}}\right\| ^{2}</script><ul>
<li>$μ^1$,$μ^2$,…,$μ^k$ 表示聚类中心</li>
<li>$c^{(1)}$,$c^{(2)}$,…,$c^{(m)}$ 存储与第 $i$ 个实例数据最近的聚类中心的索引</li>
<li>$\mu_{c^{(i)}}$代表与$x^{(i)}$最近的聚类中心点</li>
</ul>
<h4 id="13-4-Random-Initialization"><a href="#13-4-Random-Initialization" class="headerlink" title="13.4 Random Initialization"></a>13.4 Random Initialization</h4><p><strong>K-均值</strong>的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。为了解决这个问题，我们通常需要多次运行<strong>K-均值</strong>算法，每一次都重新进行随机初始化，最后再比较多次运行<strong>K-均值</strong>的结果，选择代价函数最小的结果。</p>
<p>这种方法在$K$较小的时候（2 to 10）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。</p>
<h4 id="13-5-Choosing-the-Number-of-Clusters"><a href="#13-5-Choosing-the-Number-of-Clusters" class="headerlink" title="13.5 Choosing the Number of Clusters"></a>13.5 Choosing the Number of Clusters</h4><p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用<strong>K-均值</strong>算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p>
<p>当人们在讨论选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变$K$值，也就是聚类类别数目的总数，然后计算成本函数或者计算畸变函数$J$。随着聚类数目的变大，如果畸变函数下降的速度突然减缓时（在图上表现得像一个凸出的肘部），就应该选择变缓前的最后一个值作为数目。</p>
<h4 id="13-6-其他参考资料"><a href="#13-6-其他参考资料" class="headerlink" title="13.6 其他参考资料"></a>13.6 其他参考资料</h4><h5 id="1-相似度-距离计算方法总结"><a href="#1-相似度-距离计算方法总结" class="headerlink" title="1.相似度/距离计算方法总结"></a>1.相似度/距离计算方法总结</h5><ol>
<li><p>闵可夫斯基距离 <strong>Minkowski</strong>（其中欧式距离：$p=2$) :</p>
<script type="math/tex; mode=display">
\operatorname{dist}(X, Y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{\frac{1}{p}}</script></li>
<li><p>杰卡德相似系数 <strong>Jaccard</strong>：</p>
<script type="math/tex; mode=display">
J(A,B)=\frac{\left| A\cap B \right|}{\left|A\cup B \right|}</script></li>
<li><p>余弦相似度 <strong>cosine similarity</strong>：$n$维向量$x$和$y$的夹角记做$\theta$，根据余弦定理，其余弦值为：</p>
<script type="math/tex; mode=display">
\cos (\theta)=\frac{x^{T} y}{|x| \cdot|y|}=\frac{\sum_{i=1}^{n} x_{i} y_{i}}{\sqrt{\sum_{i=1}^{n} x_{i}^{2}} \sqrt{\sum_{i=1}^{n} y_{i}^{2}}}</script></li>
<li><p><strong>Pearson</strong> 皮尔逊相关系数：</p>
<script type="math/tex; mode=display">
\rho_{X Y}=\frac{\operatorname{cov}(X, Y)}{\sigma_{X} \sigma_{Y}}=\frac{E\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right]}{\sigma_{X} \sigma_{Y}}=\frac{\sum_{i=1}^{n}\left(x-\mu_{X}\right)\left(y-\mu_{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(x-\mu_{X}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(y-\mu_{Y}\right)^{2}}}</script><p>Pearson相关系数即将$x$、$y$坐标向量各自平移到原点后的夹角余弦。</p>
</li>
</ol>
<h5 id="2-聚类的衡量指标"><a href="#2-聚类的衡量指标" class="headerlink" title="2.聚类的衡量指标"></a>2.聚类的衡量指标</h5><ol>
<li><p>均一性：$p$</p>
<p>类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个聚簇中正确分类的样本数占该聚簇总样本数的比例和)</p>
</li>
<li><p>完整性：$r$</p>
<p>类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该 类型的总样本数比例的和</p>
</li>
<li><p><strong>V-measure</strong>:</p>
<p>均一性和完整性的加权平均 </p>
<script type="math/tex; mode=display">
V=\frac{\left(1+\beta^{2}\right) * p r}{\beta^{2} * p+r}</script></li>
<li><p>轮廓系数 : $s$</p>
<p>簇内不相似度: 计算样本$i$到同簇其它样本的平均距离为$a(i)$，应尽可能小。</p>
<p>簇间不相似度: 计算样本$i$到其它簇$C_j$的所有样本的平均距离$b_{ij}$，应尽可能大。</p>
<p>轮廓系数：$s(i)$值越接近1表示样本$i$聚类越合理，越接近-1，表示样本$i$应该分类到 另外的簇中，近似为0，表示样本$i$应该在边界上;所有样本的$s(i)$的均值被成为聚类结果的轮廓系数。 </p>
<script type="math/tex; mode=display">
s(i) = \frac{b(i)-a(i)}{max\{a(i),b(i)\}}</script></li>
<li><p>调兰德指数 : <strong>ARI</strong> </p>
<p>$a_i$为实际上是 $i$ 这个组，并且聚类成了 $i$ 组的数量</p>
<p>$n_{ij}$为实际是 $i$ 组，被聚类成了 $j$ 组的数量</p>
<script type="math/tex; mode=display">
A R I=\frac{\sum_{i j}\left(\begin{array}{c}n_{i j} \\ 2\end{array}\right)-\left[\sum_{i}\left(\begin{array}{c}a_{i} \\ 2\end{array}\right) \sum_{j}\left(\begin{array}{c}b_{j} \\ 2\end{array}\right)\right] /\left(\begin{array}{c}n \\ 2\end{array}\right)}{\frac{1}{2}\left[\sum_{i}\left(\begin{array}{c}a_{i} \\ 2\end{array}\right)+\sum_{j}\left(\begin{array}{c}b_{j} \\ 2\end{array}\right)\right]-\left[\sum_{i}\left(\begin{array}{c}a_{i} \\ 2\end{array}\right) \sum_{j}\left(\begin{array}{c}b_{j} \\ 2\end{array}\right)\right] /\left(\begin{array}{c}n \\ 2\end{array}\right)}</script></li>
</ol>
<h4 id="13-7-K-means-in-Image-Compression"><a href="#13-7-K-means-in-Image-Compression" class="headerlink" title="13.7 K-means in Image Compression"></a>13.7 K-means in Image Compression</h4><p><img src="/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-2.png" srcset="/img/loading.gif" lazyload alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> skimage <span class="hljs-keyword">import</span> io<br><br>pic = io.imread(<span class="hljs-string">&#x27;./pic13-2.png&#x27;</span>) / <span class="hljs-number">255</span><br>io.imshow(pic) <span class="hljs-comment"># (128, 128, 3)</span><br>data = pic.reshape(<span class="hljs-number">128</span>*<span class="hljs-number">128</span>, <span class="hljs-number">3</span>)<br><br>C, centroids, cost_ = k_means(pd.DataFrame(data), <span class="hljs-number">16</span>, epoch=<span class="hljs-number">10</span>, n_init=<span class="hljs-number">3</span>)<br>compressed_pic = centroids[C].reshape((<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">3</span>))<br>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>ax[<span class="hljs-number">0</span>].imshow(pic)<br>ax[<span class="hljs-number">1</span>].imshow(compressed_pic)<br>plt.show()<br></code></pre></td></tr></table></figure>
<p><img src="/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_3/pic13-3.jpg" srcset="/img/loading.gif" lazyload alt></p>
<h5 id="【Sklearn-Kmeans】-1"><a href="#【Sklearn-Kmeans】-1" class="headerlink" title="【Sklearn Kmeans】"></a>【Sklearn Kmeans】</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> KMeans<br><br>model = KMeans(n_clusters=<span class="hljs-number">16</span>, n_init=<span class="hljs-number">100</span>, n_jobs=-<span class="hljs-number">1</span>)<br>model.fit(data)<br>centroids = model.cluster_centers_<br>C = model.predict(data)<br>compressed_pic = centroids[C].reshape((<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">3</span>))<br>fig, ax = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>ax[<span class="hljs-number">0</span>].imshow(pic)<br>ax[<span class="hljs-number">1</span>].imshow(compressed_pic)<br>plt.show()<br></code></pre></td></tr></table></figure>
<h3 id="14-Dimensionality-Reduction"><a href="#14-Dimensionality-Reduction" class="headerlink" title="14. Dimensionality Reduction"></a>14. Dimensionality Reduction</h3><h4 id="14-1-Motivation"><a href="#14-1-Motivation" class="headerlink" title="14.1 Motivation"></a>14.1 Motivation</h4><p>有几个不同的原因使我们可能需要做降维。一是数据压缩，它让我们加快我们的学习算法。二是数据可视化，在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。</p>
<h4 id="14-2-Principal-Component-Analysis-Problem-Formulation"><a href="#14-2-Principal-Component-Analysis-Problem-Formulation" class="headerlink" title="14.2 Principal Component Analysis Problem Formulation"></a>14.2 Principal Component Analysis Problem Formulation</h4><p>主成分分析(<strong>PCA</strong>)是最常见的降维算法。在<strong>PCA</strong>中，我们要做的是找到一个方向向量（<strong>Vector direction</strong>），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p>
<p>主成分分析与线性回归是两种不同的算法。主成分分析最小化的是垂直到向量方向的<strong>投射</strong>误差（<strong>Projected Error</strong>），而线性回归尝试的是最小化 竖直方向的<strong>预测</strong>误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p>
<p><strong>PCA</strong>技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p><strong>PCA</strong>技术的一个很大的优点是，它是完全无参数限制的。在<strong>PCA</strong>的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。</p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<h4 id="14-3-Principal-Component-Analysis-Algorithm"><a href="#14-3-Principal-Component-Analysis-Algorithm" class="headerlink" title="14.3 Principal Component Analysis Algorithm"></a>14.3 Principal Component Analysis Algorithm</h4><p><strong>PCA</strong> 从$n$维减少到$k$维的例子：</p>
<ul>
<li><p>第一步是均值归一化。我们需要计算出特征的均值，然后令 $x_j= x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ^2$。</p>
</li>
<li><p>第二步是计算<strong>协方差矩阵</strong>（<strong>covariance matrix</strong>）$Σ$：</p>
<script type="math/tex; mode=display">
\sum=\dfrac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}</script></li>
<li><p>第三步是计算协方差矩阵$Σ$的<strong>特征向量</strong>（<strong>eigenvectors</strong>）$U$。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量$z^{(i)}$:</p>
<script type="math/tex; mode=display">
z^{(i)}=U^{T}_{reduce}*x^{(i)}</script><p>其中$x$是$n×1$维的，因此结果为$k×1$维度。</p>
</li>
</ul>
<h4 id="14-4-Choosing-The-Number-Of-Principal-Components"><a href="#14-4-Choosing-The-Number-Of-Principal-Components" class="headerlink" title="14.4 Choosing The Number Of Principal Components"></a>14.4 Choosing The Number Of Principal Components</h4><p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。</p>
<p>如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。</p>
<p>我们可以先令$k=1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k=2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值</p>
<p>在求<strong>svd奇异值分解</strong>时我们有式子</p>
<script type="math/tex; mode=display">
\Sigma=USV^{T}</script><blockquote>
<p>其中 $UU^T=I$，更多相关见 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42722197/article/details/120858115">连接</a></p>
</blockquote>
<p>其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：</p>
<script type="math/tex; mode=display">
\dfrac {\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{\left( i\right) }-x^{\left( i\right) }_{approx}\right\| ^{2}}{\dfrac {1}{m}\sum^{m}_{i=1}\left\| x^{(i)}\right\| ^{2}}=1-\dfrac {\Sigma^{k}_{i=1}S_{ii}}{\Sigma^{m}_{i=1}S_{ii}}\leq 1\%</script><p>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：<script type="math/tex">x^{\left( i\right) }_{approx}=U_{reduce}z^{(i)}</script></p>
<h5 id="【Python-代码】-1"><a href="#【Python-代码】-1" class="headerlink" title="【Python 代码】"></a>【Python 代码】</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">covariance_matrix</span>(<span class="hljs-params">X</span>):<br>    m = X.shape[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">return</span> (X.T @ X) / m<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">PCA</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        X ndarray(m, n)</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        U ndarray(n, n): principle components</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    X_norm = normalize(X)<br>    Sigma = covariance_matrix(X_norm)<br>    U, S, V = np.linalg.svd(Sigma)<br>    <br>    <span class="hljs-keyword">return</span> U, S, V<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">project_data</span>(<span class="hljs-params">X, U, k</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    projected n dim to k dim</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    m, n = X.shape<br>    <span class="hljs-keyword">if</span> k &gt; n:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;k should be lower dimension of n&#x27;</span>)<br>    <br>    <span class="hljs-keyword">return</span> X @ U[:, :k]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">recover_data</span>(<span class="hljs-params">Z, U</span>):<br>    m, n = Z.shape<br>    <span class="hljs-keyword">if</span> n &gt;= U.shape[<span class="hljs-number">0</span>]:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Z dimension is upper than U, you should recover lower dimension to higher&#x27;</span>)<br>    <br>    <span class="hljs-keyword">return</span> Z @ U[:, :n].T<br><br><span class="hljs-comment"># 画投影</span><br>fig, (ax1, ax2, ax3) = plt.subplots(ncols=<span class="hljs-number">3</span>, figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">4</span>))<br>sns.rugplot(Z, ax=ax1)<br>ax1.set_title(<span class="hljs-string">&#x27;project dimension&#x27;</span>)<br>ax1.set_xlabel(<span class="hljs-string">&#x27;Z&#x27;</span>)<br><br>sns.regplot(<span class="hljs-string">&#x27;X1&#x27;</span>, <span class="hljs-string">&#x27;X2&#x27;</span>, data=pd.DataFrame(X_recover, columns=[<span class="hljs-string">&#x27;X1&#x27;</span>, <span class="hljs-string">&#x27;X2&#x27;</span>]),<br>           fit_reg=<span class="hljs-literal">False</span>, ax=ax2)<br>ax2.set_title(<span class="hljs-string">&#x27;2D projection from Z&#x27;</span>)<br><br>sns.regplot(<span class="hljs-string">&#x27;X1&#x27;</span>, <span class="hljs-string">&#x27;X2&#x27;</span>, data=pd.DataFrame(X_norm, columns=[<span class="hljs-string">&#x27;X1&#x27;</span>, <span class="hljs-string">&#x27;X2&#x27;</span>]),<br>           fit_reg=<span class="hljs-literal">False</span>, ax=ax3)<br>ax3.set_title(<span class="hljs-string">&#x27;Original dimension&#x27;</span>)<br><br>ax.plot([X_recover[:,<span class="hljs-number">0</span>], X_norm[:,<span class="hljs-number">0</span>]], [X_recover[:,<span class="hljs-number">1</span>], X_norm[:,<span class="hljs-number">1</span>]], <span class="hljs-string">&#x27;--&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h5 id="【Sklearn-Kmeans】-2"><a href="#【Sklearn-Kmeans】-2" class="headerlink" title="【Sklearn Kmeans】"></a>【Sklearn Kmeans】</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA <span class="hljs-keyword">as</span> sk_PCA<br><br>pca = sk_PCA(n_components=<span class="hljs-number">100</span>)<br>Z = pca.fit_transform(X)<br></code></pre></td></tr></table></figure>
<h4 id="14-5-Advice-for-Applying-PCA"><a href="#14-5-Advice-for-Applying-PCA" class="headerlink" title="14.5 Advice for Applying PCA"></a>14.5 Advice for Applying PCA</h4><p>一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息。因此如果你在监督学习中采用PCA可能会丢失非常重要的性质。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p>
<p>另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>
<h2 id="Week-9-Anomaly-Detection"><a href="#Week-9-Anomaly-Detection" class="headerlink" title="Week 9 : Anomaly Detection"></a>Week 9 : Anomaly Detection</h2><h3 id="15-Anomaly-Detection"><a href="#15-Anomaly-Detection" class="headerlink" title="15. Anomaly Detection"></a>15. Anomaly Detection</h3><h4 id="15-1-Problem-Motivation"><a href="#15-1-Problem-Motivation" class="headerlink" title="15.1 Problem Motivation"></a>15.1 Problem Motivation</h4><p>所谓的异常检测问题就是：我们希望知道这个新的数据是否有某种异常，它是否符合我们模型应该得到的结果。</p>
<p>我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 $p(x)$。</p>
<p>这种方法称为密度估计，表达如下：</p>
<script type="math/tex; mode=display">
if \quad p(x)
\begin{cases}
< \varepsilon & anomaly \\
> =\varepsilon & normal
\end{cases}</script><h4 id="15-2-Gaussian-Distribution"><a href="#15-2-Gaussian-Distribution" class="headerlink" title="15.2 Gaussian Distribution"></a>15.2 Gaussian Distribution</h4><p>回顾高斯分布的基本知识。</p>
<p>通常如果我们认为变量 $x$ 符合高斯分布 $x \sim N(\mu, \sigma^2)$则其概率密度函数为：</p>
<script type="math/tex; mode=display">
p(x,\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)</script><p>我们可以利用已有的数据来预测总体中的$μ$和$σ^2$的计算方法如下：</p>
<script type="math/tex; mode=display">
\mu=\frac{1}{m}\sum\limits_{i=1}^{m}x^{(i)}</script><script type="math/tex; mode=display">
\sigma^2=\frac{1}{m}\sum\limits_{i=1}^{m}(x^{(i)}-\mu)^2</script><blockquote>
<p> 机器学习中对于方差我们通常只除以$m$而非统计学中的$(m-1)$</p>
</blockquote>
<h4 id="15-3-Algorithm"><a href="#15-3-Algorithm" class="headerlink" title="15.3 Algorithm"></a>15.3 Algorithm</h4><p>一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：</p>
<script type="math/tex; mode=display">
p(x)=\prod\limits_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod\limits_{j=1}^1\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})</script><p>当$p(x) &lt; \varepsilon$时，为异常。</p>
<h5 id="【Python-代码】-2"><a href="#【Python-代码】-2" class="headerlink" title="【Python 代码】"></a>【Python 代码】</h5><p>单变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">mu = X.mean(axis=<span class="hljs-number">0</span>)<br>sigma2 = ((X - mu)**<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>) / X.shape[<span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pdf</span>(<span class="hljs-params">x, mu, sigma2</span>):<br>    m, n = x[<span class="hljs-number">0</span>].shape<br>    ret = <span class="hljs-number">1</span> / np.power(<span class="hljs-number">2</span>*np.pi*sigma2, n/<span class="hljs-number">2</span>) * np.exp(-(x-mu)*(x-mu) / (<span class="hljs-number">2</span>*sigma2))<br>    ret = ret.prod(axis=<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span>  ret<br><br><span class="hljs-comment"># 画等高线</span><br>x, y = np.mgrid[<span class="hljs-number">0</span>:<span class="hljs-number">30</span>:<span class="hljs-number">0.01</span>, <span class="hljs-number">0</span>:<span class="hljs-number">30</span>:<span class="hljs-number">0.01</span>]<br>pos = np.dstack((x, y))<br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>ax.contourf(x, y, pdf(pos, mu, sigma2), cmap=<span class="hljs-string">&#x27;Blues&#x27;</span>)<br>ax.contour(x, y, pdf(pos, mu, sigma2), colors=<span class="hljs-string">&#x27;black&#x27;</span>)<br>ax.scatter(X[:, <span class="hljs-number">0</span>], X[:, <span class="hljs-number">1</span>])<br><br>plt.show()<br></code></pre></td></tr></table></figure>
<h4 id="15-4-Developing-and-Evaluating-an-Anomaly-Detection-System"><a href="#15-4-Developing-and-Evaluating-an-Anomaly-Detection-System" class="headerlink" title="15.4 Developing and Evaluating an Anomaly Detection System"></a>15.4 Developing and Evaluating an Anomaly Detection System</h4><p>异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 $ y$ 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。</p>
<p>例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据：</p>
<p>6000台正常引擎的数据作为训练集</p>
<p>2000台正常引擎和10台异常引擎的数据作为交叉检验集</p>
<p>2000台正常引擎和10台异常引擎的数据作为测试集</p>
<p>具体的评价方法如下：</p>
<ol>
<li>根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数</li>
<li>对交叉检验集，我们尝试使用不同的$\varepsilon$值作为阀值，并预测数据是否异常，根据$F1$值或者查准率与查全率的比例来选择 $\varepsilon$</li>
<li>选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的$F1$值，或者查准率与查全率之比</li>
</ol>
<h4 id="15-5-Anomaly-Detection-vs-Supervised-Learning"><a href="#15-5-Anomaly-Detection-vs-Supervised-Learning" class="headerlink" title="15.5 Anomaly Detection vs. Supervised Learning"></a>15.5 Anomaly Detection vs. Supervised Learning</h4><p>之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测：</p>
<p>两者比较：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>异常检测</th>
<th>监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td>非常少量的正向类（异常数据 $y=1$）, 大量的负向类（$y=0$）</td>
<td>同时有大量的正向类和负向类</td>
</tr>
<tr>
<td>根据非常少量的正向类数据来训练算法。</td>
<td>有足够多的正向类实例，足够用于训练算法。</td>
</tr>
<tr>
<td>未来遇到的异常可能与已掌握的异常、非常的不同。</td>
<td>未来遇到的正向类实例可能与训练集中的非常近似。</td>
</tr>
<tr>
<td>例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况</td>
<td>例如：邮件过滤器 天气预报 肿瘤分类</td>
</tr>
</tbody>
</table>
</div>
<h4 id="15-6-Choosing-What-Features-to-Use"><a href="#15-6-Choosing-What-Features-to-Use" class="headerlink" title="15.6 Choosing What Features to Use"></a>15.6 Choosing What Features to Use</h4><p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：$x= log(x+c)$，其中 $c$ 为非负常数； 或者 $x=x^c$，$c$为 0-1 之间的一个分数，等方法。</p>
<p>一个常见的问题是一些异常的数据可能也会有较高的$p(x)$值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。</p>
<h4 id="15-7-Multivariate-Gaussian-Distribution"><a href="#15-7-Multivariate-Gaussian-Distribution" class="headerlink" title="15.7 Multivariate Gaussian Distribution"></a>15.7 Multivariate Gaussian Distribution</h4><p>在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 $p(x)$。</p>
<p>我们首先计算所有特征的平均值，然后再计算协方差矩阵：</p>
<script type="math/tex; mode=display">
\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}</script><script type="math/tex; mode=display">
\Sigma = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\frac{1}{m}(X-\mu)^T(X-\mu)</script><p>注:其中$\mu $ 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的$p\left( x \right)$:</p>
<script type="math/tex; mode=display">
p(x)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)</script><p>原高斯分布模型和多元高斯分布模型的比较：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原高斯分布模型</th>
<th>多元高斯分布模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决</td>
<td>自动捕捉特征之间的相关性</td>
</tr>
<tr>
<td>计算代价低，能适应大规模的特征</td>
<td>计算代价较高 训练集较小时也同样适用</td>
</tr>
<tr>
<td></td>
<td>必须要有 $m&gt;n$，不然的话协方差矩阵$\Sigma$不可逆的，通常需要 $m&gt;10n$ 另外特征冗余也会导致协方差矩阵不可逆</td>
</tr>
</tbody>
</table>
</div>
<p>原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。</p>
<p>如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。</p>
<h5 id="【Python-代码】-3"><a href="#【Python-代码】-3" class="headerlink" title="【Python 代码】"></a>【Python 代码】</h5><p>多变量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> stats<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score, classification_report<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">select_threshold</span>(<span class="hljs-params">X, Xval, yval</span>):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    use CV data to find the best epsilon</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        e: best epslion with the highest F-score</span><br><span class="hljs-string">        f-score</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <span class="hljs-comment"># create multivariate model using traing data</span><br>    mu = X.mean(axis=<span class="hljs-number">0</span>)<br>    cov = np.cov(X.T)<br>    multi_normal = stats.multivariate_normal(mu, cov)<br>    <br>    pval = multi_normal.pdf(Xval)<br>    epsilon = np.linspace(np.<span class="hljs-built_in">min</span>(pval), np.<span class="hljs-built_in">max</span>(pval), num=<span class="hljs-number">10000</span>)<br>    <br>    fs = []<br>    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> epsilon:<br>        y_pred = (pval &lt;= e).astype(<span class="hljs-string">&#x27;int&#x27;</span>)<br>        fs.append(f1_score(yval, y_pred))<br>        <br>    argmax_fs = np.argmax(fs)<br>    <br>    <span class="hljs-keyword">return</span> epsilon[argmax_fs], fs[argmax_fs]   <br><br>e, fs = select_threshold(X, Xval, yval)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Best epsilon: &#123;&#125;\nBest F-score on validation data: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(e, fs))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span> (X, Xval, e, Xtest, ytest):<br>    <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        multi_normal: multivariate normal model</span><br><span class="hljs-string">        y_pred: prediction of test data</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br>    <br>    Xdata = np.concatenate((X, Xval), axis=<span class="hljs-number">0</span>)<br>    mu = Xdata.mean(axis=<span class="hljs-number">0</span>)<br>    cov = np.cov(Xdata.T)<br>    multi_normal = stats.multivariate_normal(mu, cov)<br>    <br>    pval = multi_normal.pdf(Xtest)<br>    y_pred = (pval &lt;= e).astype(<span class="hljs-string">&#x27;int&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(classification_report(ytest, y_pred))<br>    <br>    <span class="hljs-keyword">return</span> multi_normal, y_pred<br><br>data = pd.DataFrame(Xtest, columns=[<span class="hljs-string">&#x27;Latency&#x27;</span>, <span class="hljs-string">&#x27;Throughput&#x27;</span>])<br>data[<span class="hljs-string">&#x27;y_pred&#x27;</span>] = y_pred<br><br>fig, ax = plt.subplots(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>ax.contourf(x, y, multi_normal.pdf(pos), cmap=<span class="hljs-string">&#x27;Blues&#x27;</span>)<br>ax.contour(x, y, multi_normal.pdf(pos), colors=<span class="hljs-string">&#x27;black&#x27;</span>)<br>ax.scatter(Xtest[:, <span class="hljs-number">0</span>], Xtest[:, <span class="hljs-number">1</span>], c=<span class="hljs-string">&#x27;b&#x27;</span>, s=<span class="hljs-number">10</span>)<br><br>anamoly_data = data[data[<span class="hljs-string">&#x27;y_pred&#x27;</span>] == <span class="hljs-number">1</span>]<br>ax.scatter(anamoly_data[<span class="hljs-string">&#x27;Latency&#x27;</span>], anamoly_data[<span class="hljs-string">&#x27;Throughput&#x27;</span>], marker=<span class="hljs-string">&#x27;x&#x27;</span>, s=<span class="hljs-number">50</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure>
<blockquote>
<p>from sklearn.model_selection import train_test_split 自动分成两部分</p>
</blockquote>
<h3 id="16-Recommender-Systems"><a href="#16-Recommender-Systems" class="headerlink" title="16. Recommender Systems"></a>16. Recommender Systems</h3><p>此部分在 <a href="https://achlier.github.io/2022/03/08/Recommender-System_Classical-RS/">Recommender System:Classical RS</a> 有更详细的笔记，在此忽略。</p>
<h2 id="Week-10-Large-Scale-Machine-Learning"><a href="#Week-10-Large-Scale-Machine-Learning" class="headerlink" title="Week 10 : Large Scale Machine Learning"></a>Week 10 : Large Scale Machine Learning</h2><h3 id="17-Gradient-Descent-with-Large-Database"><a href="#17-Gradient-Descent-with-Large-Database" class="headerlink" title="17. Gradient Descent with Large Database"></a>17. Gradient Descent with Large Database</h3><h4 id="17-1-Stochastic-Gradient-Descent"><a href="#17-1-Stochastic-Gradient-Descent" class="headerlink" title="17.1 Stochastic Gradient Descent"></a>17.1 Stochastic Gradient Descent</h4><p>如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。</p>
<p>在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：</p>
<script type="math/tex; mode=display">
\operatorname{cost}\left(\theta,\left(x^{(i)}, y^{(i)}\right)\right)=\frac{1}{2}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p><strong>随机</strong>梯度下降算法为：首先对训练集随机“洗牌”，然后：</p>
<p> <strong>Repeat</strong> (usually anywhere between1-10){</p>
<p>  <strong>for</strong> $i = 1:m${</p>
<p>​        $\theta:=\theta_{j}-\alpha\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}$      </p>
<p>​        (<strong>for</strong> $j=0:n$)</p>
<p> ​    }<br> }</p>
<p>随机梯度下降算法在每一次计算之后便更新参数 $\theta $ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。</p>
<h4 id="17-2-Mini-Batch-Gradient-Descent"><a href="#17-2-Mini-Batch-Gradient-Descent" class="headerlink" title="17.2 Mini-Batch Gradient Descent"></a>17.2 Mini-Batch Gradient Descent</h4><p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数  $\theta $ 。</p>
<p> <strong>Repeat</strong> {</p>
<p> <strong>for</strong> $i = 1:m${</p>
<p>​        $\theta:=\theta_{j}-\alpha \frac{1}{b} \sum_{k=i}^{i+b-1}\left(h_{\theta}\left(x^{(k)}\right)-y^{(k)}\right) x_{j}^{(k)}$      </p>
<p>​       (<strong>for</strong> $j=0:n$)</p>
<p>​      $ i +=10 $   </p>
<p> ​     }<br> }</p>
<p>通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p>
<h4 id="17-3-Stochastic-Gradient-Descent-Convergence"><a href="#17-3-Stochastic-Gradient-Descent-Convergence" class="headerlink" title="17.3 Stochastic Gradient Descent Convergence"></a>17.3 Stochastic Gradient Descent Convergence</h4><p>在随机梯度下降中，我们在每一次更新 $\theta $ <strong>之前</strong>都计算一次代价，然后每$x$次迭代后，求出这$x$次对训练实例计算代价的平均值，然后绘制这些平均值与$x$次迭代的次数之间的函数图表。</p>
<p>当我们绘制图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像。我们可以增加$\alpha$来使得函数更加平缓，也许便能看出下降的趋势了；对于函数图表仍然是颠簸不平且不下降的，那么我们的模型本身可能存在一些错误。</p>
<p>如果我们得到的曲线不断地上升，那么我们可能会需要选择一个较小的学习率$\alpha$。我们也可以令学习率随着迭代次数的增加而减小，例如令：</p>
<script type="math/tex; mode=display">
\alpha=\frac{\text { const } 1}{\text { IterationNumber }+\text { const } 2}</script><h4 id="17-4-Online-Learning"><a href="#17-4-Online-Learning" class="headerlink" title="17.4 Online Learning"></a>17.4 Online Learning</h4><p>今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果有一个由连续的用户流引发的连续的数据流进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。</p>
<p>在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。每次交互事件并不只产生一个数据集，例如，我们一次给用户提供3个物流选项，用户选择2项，我们实际上可以获得3个新的训练实例，因而我们的算法可以一次从3个实例中学习并更新模型。</p>
<h4 id="17-5-Map-Reduce-and-Data-Parallelism"><a href="#17-5-Map-Reduce-and-Data-Parallelism" class="headerlink" title="17.5 Map Reduce and Data Parallelism"></a>17.5 Map Reduce and Data Parallelism</h4><p>映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。</p>
<p>具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同<strong>CPU</strong> 核心），以达到加速处理的目的。</p>
<h2 id="Week-11-Application-Example-Photo-OCR"><a href="#Week-11-Application-Example-Photo-OCR" class="headerlink" title="Week 11 : Application Example : Photo OCR"></a>Week 11 : Application Example : Photo OCR</h2><h3 id="18-Photo-OCR"><a href="#18-Photo-OCR" class="headerlink" title="18. Photo OCR"></a>18. Photo OCR</h3><h4 id="18-1-Problem-Description-and-Pipeline"><a href="#18-1-Problem-Description-and-Pipeline" class="headerlink" title="18.1 Problem Description and Pipeline"></a>18.1 Problem Description and Pipeline</h4><p>图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。</p>
<p>为了完成这样的工作，需要采取如下步骤：</p>
<ol>
<li><p>文字侦测（<strong>Text detection</strong>）——将图片上的文字与其他环境对象分离开来</p>
</li>
<li><p>字符切分（<strong>Character segmentation</strong>）——将文字分割成一个个单一的字符</p>
</li>
<li><p>字符分类（<strong>Character classification</strong>）——确定每一个字符是什么</p>
</li>
</ol>
<h4 id="18-2-Sliding-Windows"><a href="#18-2-Sliding-Windows" class="headerlink" title="18.2 Sliding Windows"></a>18.2 Sliding Windows</h4><p>滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。然后，我们按比例放大剪裁的区域，以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。</p>
<p>滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。</p>
<p>最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。</p>
<h4 id="18-3-Getting-Lots-of-Data-and-Artificial-Data"><a href="#18-3-Getting-Lots-of-Data-and-Artificial-Data" class="headerlink" title="18.3 Getting Lots of Data and Artificial Data"></a>18.3 Getting Lots of Data and Artificial Data</h4><p>如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。</p>
<p>以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。</p>
<p>另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。</p>
<p>有关获得更多数据的几种方法：</p>
<pre><code class="hljs">1. 人工数据合成

2. 手动收集、标记数据

3. 众包
</code></pre><h4 id="18-4-Ceiling-Analysis-What-Part-of-the-Pipeline-to-Work-on-Next"><a href="#18-4-Ceiling-Analysis-What-Part-of-the-Pipeline-to-Work-on-Next" class="headerlink" title="18.4 Ceiling Analysis_ What Part of the Pipeline to Work on Next"></a>18.4 Ceiling Analysis_ What Part of the Pipeline to Work on Next</h4><p>在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。</p>
<p>流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。</p>
<p>如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。</p>
<p>接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。</p>
<p>最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。</p>
<h3 id="19-致谢"><a href="#19-致谢" class="headerlink" title="19. 致谢"></a>19. 致谢</h3><p>至此《机器学习》的课程就全部结束了。非常感谢 Dr. Andrew Ng 带我入了 Machine Learning 这个大坑，让我接触到了许多大学课程所学不到的知识。同时这门课程的内容确实非常适合没有基础的小白入门，它甚至包含了一些数学基础复习。并且课后的练习让在课上没有完全听懂的我得以抓住每节课的知识点，在课后理解复习。令人欣喜的是这门课同时还传授了许多构建模型实战运用的技巧，虽然在几年前第一次听这门课时并没有太过注意，但当现在的我已经有足够的基础做过一些研究后，才发现这些技巧和经验是十分可贵的！最后再次感谢教授耐心的指导和精彩的教学，以及每节课上的灿烂笑容！</p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Machine-Learning/" class="category-chain-item">Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【课程】吴恩达机器学习课程(三)</div>
      <div>http://achlier.github.io/2022/08/05/吴恩达机器学习课程_3/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Hailey</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>August 5, 2022</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>Licensed under</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/08/05/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B_2/" title="【课程】吴恩达机器学习课程(二)">
                        <span class="hidden-mobile">【课程】吴恩达机器学习课程(二)</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>






  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
